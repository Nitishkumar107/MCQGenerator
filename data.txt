
2
1. What is Generative AI? 
 
Generative AI, short for Generative Artificial Intelligence, is a subset of artificial intelligence 
(AI) that focuses on enabling machines to produce content or data that resembles human-
generated information. It’s a technology that’s gaining immense popularity in various fields, 
from natural language processing to creative content generation. 
 
Generative AI operates on a principle of learning patterns from existing data and using that 
knowledge to create new content. It relies on deep learning techniques, particularly neural 
networks, to accomplish this task. These neural networks are trained on large datasets, 
allowing them to generate text, images, music, and more. 
 
 
2. How does Generative AI work? 
 
Generative AI works through the use of neural networks, specifically Recurrent Neural 
Networks (RNNs) and more recently, Transformers. Here’s a simplified breakdown of how it 
functions: 
 
● Data Collection: To begin, a substantial amount of data related to the specific task is 
gathered. For instance, if you want to generate text, the model needs a massive text 
corpus to learn from. 
● Training: The neural network is then trained on this data. During training, the model 
learns the underlying patterns, structures, and relationships within the data. It learns 
to predict the next word, character, or element in a sequence. 
● Generation: Once trained, the model can generate content by taking a seed input 
and predicting the subsequent elements. For instance, if you give it the start of a 
sentence, it can complete the sentence in a coherent and contextually relevant 
manner. 
● Fine-Tuning: Generative AI models can be further fine-tuned for specific tasks or 
domains to improve the quality of generated content. 
 
 
3. What are the top applications of Generative AI? 
 
Generative AI has a wide range of applications across different industries: 
 
● Natural Language Processing (NLP): It’s used for text generation, language 
translation, and chatbots that can engage in human-like conversations. 
● Content Generation: Generative AI can create articles, stories, and even poetry. It’s 
used by content creators to assist in writing. 
● Image and Video Generation: It can generate realistic images and videos, which are 
valuable in fields like entertainment and design. 
● Data Augmentation: In data science, it’s used to create synthetic data for training 
machine learning models. 
● Healthcare: Generative AI helps in generating medical reports, simulating disease 
progression, and drug discovery. 
 
4. Can you explain the difference between Generative AI and Discriminative AI? 
 
Generative AI 
 
● Generates new data based on learned patterns. 
● Content creation, data generation. 
● Text generation, image synthesis, creativity. 
● Learn patterns in data for content generation. 
● Chatbots, text generators, art creation. 
 
Discriminative AI 
 
● Classifies input data into predefined classes. 
● Classification, discrimination. 
● Spam detection, sentiment analysis, image recognition. 
● Focuses on learning boundaries between classes. 
● Spam filters, image classifiers, sentiment analysis models. 
 
 
5. What are some popular Generative AI models? 
 
Generative AI models have revolutionised the field of artificial intelligence, offering 
remarkable capabilities in generating content, from text to images and beyond. In this 
section, we’ll explore some of the most popular and influential Generative AI models that 
have left a significant mark on the industry. 
 
● GPT-4 (Generative Pre-trained Transformer 4): GPT-4, developed by OpenAI, is a 
standout among Generative AI models. With billions of parameters, it has 
demonstrated remarkable text generation abilities. GPT-4 can answer questions, 
write essays, generate code, and even create conversational agents that engage 
users in natural language. 
● BERT (Bidirectional Encoder Representations from Transformers): Although 
primarily known for its prowess in natural language understanding, BERT also 
exhibits generative capabilities. It excels in tasks like text completion and 
summarization, making it a valuable tool in various applications, including search 
engines and chatbots. 
● DALL·E: If you’re interested in generative art, DALL·E is a model to watch. 
Developed by OpenAI, this model can generate images from textual descriptions. It 
takes creativity to new heights by creating visuals based on written prompts, showing 
the potential of Generative AI in the visual arts. 
● StyleGAN2: When it comes to generating realistic images, StyleGAN2 is a name that 
stands out. It can create high-quality, diverse images that are virtually 
indistinguishable from real photographs. StyleGAN2 has applications in gaming, 
design, and even fashion. 
● VQ-VAE-2 (Vector Quantized Variational Autoencoder 2): This model combines 
elements of generative and variational autoencoders to generate high-quality, high-
resolution images. It has made significant strides in image compression and 
generation. 
6. How is Generative Adversarial Networks (GANs) used in AI? 
 
Generative Adversarial Networks, orGANs, have emerged as a groundbreaking concept in 
the realm of Generative AI. These networks consist of two primary components: a generator 
and a discriminator, which work in tandem to create and evaluate content.  
 
 
 
7. What are the limitations of Generative AI? 
 
While Generative AI has made remarkable strides, it’s essential to acknowledge its 
limitations and challenges. Understanding these limitations is crucial for responsible and 
effective use. Here are some key constraints of Generative AI: 
 
● Data Dependency: Generative AI models, including GANs, require vast amounts of 
data for training. Without sufficient data, the quality of generated content may suffer, 
and the model might produce unrealistic or biassed results. 
● Ethical Concerns: Generative AI can inadvertently perpetuate biases present in the 
training data. This raises ethical concerns, particularly when it comes to generating 
content related to sensitive topics, such as race, gender, or religion. 
● Lack of Control: Generative AI can be unpredictable. Controlling the output to meet 
specific criteria, especially in creative tasks, can be challenging. This lack of control 
can limit its practicality in some applications. 
● Resource Intensive: Training and running advanced Generative AI models demand 
substantial computational resources, making them inaccessible to smaller 
organisations or individuals with limited computing power. 
● Overfitting: Generative models may memorise the training data instead of learning its 
underlying patterns. This can result in content that lacks diversity and creativity. 
● Security Risks: There is the potential for malicious use of Generative AI, such as 
generating deepfake videos for deceptive purposes or creating fake content to 
spread misinformation. 
● Intellectual Property Concerns: When Generative AI is used to create content, 
determining ownership and copyright becomes complex. This raises legal questions 
about intellectual property rights. 
● Validation Challenges: It can be difficult to validate the authenticity of content 
generated by Generative AI, which can be problematic in contexts where trust and 
reliability are paramount. 
 
 
 
 
 
 
 
 
 
 
 
8. What are the challenges in training Generative AI models? 
 
Training Generative AI models presents several challenges: 
 
● Data Quality: High-quality training data is essential. Noisy or biassed data can lead to 
flawed outputs. 
● Computational Resources: Training large models demands substantial computational 
power and time. 
● Mode Collapse: GANs may suffer from mode collapse, where they generate limited 
varieties of outputs. 
● Ethical Considerations: AI-generated content can raise ethical issues, including 
misinformation and deep fakes. 
● Evaluation Metrics: Measuring the quality of generated content is subjective and 
requires robust evaluation metrics. 
 
 
9. How does text generation with Generative AI work? 
 
Text generation with Generative AI involves models like GPT (Generative Pre-trained 
Transformer). Here’s how it works: 
 
● Pre-training: Models are initially trained on a massive corpus of text data, learning 
grammar, context, and language nuances. 
● Fine-tuning: After pre-training, models are fine-tuned on specific tasks or datasets, 
making them domain-specific. 
● Autoregressive Generation: GPT generates text autoregressive, predicting the next 
word based on context. It’s conditioned on input text. 
● Sampling Strategies: Techniques like beam search or temperature-based sampling 
control the creativity and diversity of generated text. 
 
 
10. Are there any Generative AI models used in natural language processing (NLP)? 
 
Generative AI models have made significant strides in the field of Natural Language 
Processing (NLP), revolutionising the way machines understand and generate human 
language. One of the most prominent examples is the use of Transformers, a class of 
generative models that has reshaped NLP. 
 
Transformers, which includes models like GPT-4 (Generative Pre-trained Transformer 4) 
and BERT (Bidirectional Encoder Representations from Transformers), have demonstrated 
remarkable capabilities in understanding and generating natural language text. 
 
Here’s how they work: 
 
● Attention Mechanism: Transformers utilise an attention mechanism that allows them 
to weigh the importance of each word or token in a sentence concerning others. This 
mechanism helps the model capture context effectively. 
